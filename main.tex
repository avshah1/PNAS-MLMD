\documentclass[12pt,a4paper,dvipsnames]{article}

\usepackage[margin=0.5in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}

\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{float}

\usepackage{longtable}
\usepackage{threeparttable}
\usepackage{booktabs,tabularx,dcolumn}

\usepackage{tikz}
\usetikzlibrary{automata, positioning}

\usepackage{xcolor}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}

\usepackage{listings}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2}
\lstset{style=mystyle}

\usepackage{gensymb}
\usepackage{relsize}
\usepackage{parskip}

\usepackage[citestyle=authoryear,natbib=true,backend=bibtex]{biblatex}
\renewcommand\nameyeardelim{, }
\addbibresource{bibliography.bib}


\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}

\newcommand{\contradiction}{{\hbox{%
    \setbox0=\hbox{$\mkern-3mu\times\mkern-3mu$}%
    \setbox1=\hbox to0pt{\hss$\times$\hss}%
    \copy0\raisebox{0.5\wd0}{\copy1}\raisebox{-0.5\wd0}{\box1}\box0
}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother

\def\uu{\mathbf{u}}
\def\vv{\mathbf{v}}
\def\ww{\mathbf{w}}
\def\im{\mathrm{im}}
\def\H{\mathbf{H}}
\def\qbar{\overline{q}}
\def\SO{\mathrm{SO}}
\def\OO{\mathrm{O}}
\def\ZZ{\mathrm{Z}}
\def\B{\mathcal{B}}
\def\C{\mathbf{C}}
\def\E{\mathbb{E}}
\def\P{\mathbb{P}}
\def\L{\mathbb{L}}
\def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\Q{\mathbf{Q}}
\def\Z{\mathbf{Z}}
\def\X{\mathcal{X}}
\def\F{\mathbf{F}}
\def\GL{\mathrm{GL}}
\def\SL{\mathrm{SL}}
\def\PGL{\mathrm{PGL}}
\def\PSL{\mathrm{PSL}}
\def\Aut{\mathrm{Aut}}
\def\Inn{\mathrm{Inn}}
\def\Out{\mathrm{Out}}
\def\xbar{\overline{x}}
\def\ybar{\overline{y}}
\def\RO{\mathrm{RO}}
\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\comment}{\textcolor{red}}


\begin{document}

\title{Machine Learning \\for Research Design \\for Market Design (for PNAS)}
\author{Anand Shah}
\date{February 2026}

\maketitle


\section{Introduction}

A growing number of school districts use centralized assignment mechanisms --- most commonly variants of deferred acceptance (DA) --- to allocate school seats. A major payoff of centralized assignment is that it embeds quasi-experimental variation that can be used for credible impact evaluation. The Market Design for Research Design (MDRD) framework of Abdulkadiro\u{g}lu et al.\ (2017) shows how to extract this variation by deriving propensity scores $\hat{p}^{DA}$ directly from the mechanism's structure. These scores are the foundation of a substantial body of research on school effectiveness.

But this entire research program rests on an assumption: that the mechanism operates exactly as specified. If a district informally adjusts assignments after the algorithm runs, if priority structures are miscoded in the data, or if the implemented rules differ from the researcher's model of them, then $\hat{p}^{DA}$ is wrong --- and every downstream causal estimate built on it is biased. To date, this assumption has been maintained rather than tested.

We propose using machine learning to audit $\hat{p}^{DA}$. The logic is simple. Under the MDRD assumptions, a student's probability of receiving a first-round offer at a given school is fully determined by their \textit{type} $\theta_i$ --- their submitted preferences and priority status --- together with school capacities and the composition of the applicant pool. Variables outside the mechanism --- demographics, test scores, neighborhood characteristics --- should have \textit{zero} additional predictive power once we condition on $\theta$. We test this implication directly.

We develop the idea in the context of Denver Public Schools (DPS), where we observe the complete mechanism inputs needed to compute $\hat{p}^{DA}$ as well as a rich set of student-level covariates that the mechanism is not supposed to use. The paper proceeds in three steps:

\begin{enumerate}
    \item \textbf{Benchmark.} We compute $\hat{p}^{DA}$ at the bucket level from the known mechanism, following MDRD. This is our null hypothesis: offers are generated exactly by DA applied to $(\theta, \text{capacities})$.

    \item \textbf{Two ML cuts.} We train ML models to predict first-round offers in two cuts. Cut 1 uses only the mechanism's official inputs (preferences, priorities, capacities) --- everything the algorithm uses except the lottery number. Cut 2 adds exhaust covariates (demographics, test scores, distance, neighborhood characteristics) that the mechanism is not supposed to use. If the mechanism operates as designed, Cut 2 should not improve on Cut 1.

    \item \textbf{Comparison.} We compare $\hat{p}^{DA}$, $\hat{p}^{ML}_1$, and $\hat{p}^{ML}_2$ on predicting actual offer outcomes. The pattern of results tells us whether the mechanism operates as advertised, and if not, which variables are associated with the deviation.
\end{enumerate}

\noindent This framework is an audit of $\hat{p}^{DA}$ \textit{as implemented by the researcher} --- it detects any discrepancy between what the propensity score says should happen and what actually happened, regardless of the source. The result is useful whether the deviation reflects data errors, mechanism misspecification, operational discretion, or systematic undocumented dependence on student characteristics.

\section{The MDRD Benchmark: $\hat{p}^{DA}$}

The MDRD framework derives the propensity score for school assignment from the structure of the DA algorithm. Under DA, a student's assignment depends on three objects: (i) their submitted preference ranking over schools, (ii) their priority status at each school (e.g., sibling priority, neighborhood priority), and (iii) a randomly drawn lottery number used to break ties among students with equal priority. Student \textit{type} $\theta_i = (\succ_i, \rho_i)$ is defined as the combination of preferences and priorities. Conditional on type, assignment is determined entirely by the lottery draw, which is i.i.d.\ uniform and independent of all student characteristics. This is the conditional independence property:
\[
D_i(s) \perp W_i \mid \theta_i,
\]
where $D_i(s)$ indicates whether student $i$ receives a first-round offer at school $s$ and $W_i$ is any variable not determined by the lottery. By Rosenbaum and Rubin (1983), conditioning on the propensity score $p_s(\theta) = \Pr[D_i(s) = 1 \mid \theta_i = \theta]$ is sufficient to eliminate selection bias.

The key contribution of MDRD is Theorem 1, which provides a closed-form expression for $p_s(\theta)$ in the large-market limit. The score depends on a small number of intermediate quantities: marginal priority status at $s$, the lottery cutoff $\tau_s$, and the \textit{most informative disqualification} (MID) --- the most forgiving cutoff at schools the student prefers to $s$. This structure pools many distinct types into a tractable number of strata, dramatically expanding the quasi-experimental sample relative to first-choice or full-type conditioning strategies.

In practice, $\hat{p}^{DA}$ is computed either by simulation (running DA across a large number of lottery draws and recording average offer rates by type) or by plugging sample analogs of the formula inputs into Theorem 1. Both approaches require complete knowledge of preferences, priorities, and capacities. With full $\theta$ and the exact mechanism, $\hat{p}^{DA}$ is the oracle: no learned model $\hat{p}^{ML}(\theta)$ can beat it as a probability forecast of first-round offers.

\textit{Bucket-level computation.} In DPS, the DA algorithm assigns seats at the level of \textit{buckets} --- sub-schools created by seat reservation policies (e.g., a set of seats reserved for free/reduced lunch students, with the remaining seats in a general pool). Each bucket has its own capacity and priority structure. The propensity score is properly defined at the bucket level; school-level offer probabilities are obtained by summing across buckets belonging to the same school. We follow MDRD in computing $\hat{p}^{DA}$ at the bucket level throughout.

\textit{Assumptions.} The validity of $\hat{p}^{DA}$ rests on three assumptions: (a) lottery numbers are i.i.d.\ uniform and independent of type; (b) the mechanism is exactly DA as specified (including the correct variant --- single vs.\ multiple tie-breaking, seat reservations, family link rules, bucket construction); and (c) priority structures are correctly recorded in the data. Our audit tests whether $\hat{p}^{DA}$ computed under these assumptions matches observed offer outcomes.

\section{Machine Learning Propensity Scores}

We train ML models to predict first-round offers, producing two sets of propensity scores corresponding to two cuts of the covariate space.

\subsection{Cut 1: Mechanism Inputs Only ($\hat{p}^{ML}_1$)}

Cut 1 uses only the variables that the DA mechanism officially processes: the student's preference ranking, priority status at each bucket, school capacities, and the composition of the applicant pool (e.g., number of applicants by priority group at each bucket). We deliberately exclude the lottery number, since we are learning the probability of an offer, not the realization.

This cut asks: can ML learn the mechanism's own probability function from its own inputs? Under the MDRD assumptions, $p_s(\theta)$ is a deterministic function of exactly these inputs. If $\hat{p}^{ML}_1 \approx \hat{p}^{DA}$, the ML pipeline is competent and the mechanism inputs are correctly captured. If $\hat{p}^{ML}_1$ fails to match $\hat{p}^{DA}$, either the model is insufficiently flexible or the features do not fully encode $\theta$ --- a signal that the researcher's data may be incomplete before we even get to the substantive test.

\subsection{Cut 2: Mechanism Inputs Plus Exhaust ($\hat{p}^{ML}_2$)}

Cut 2 augments the Cut 1 feature set with variables the mechanism is \textit{not supposed to use}: student demographics (race, gender, free/reduced lunch status), prior test scores, neighborhood characteristics, distance to schools, and other student-level covariates available in the Denver data.

The key question is whether these additional covariates improve prediction \textit{on top of} Cut 1. Under the MDRD conditional independence property $D_i(s) \perp W_i \mid \theta_i$, these variables have zero additional predictive power once we condition on the mechanism inputs. If Cut 2 improves on Cut 1, something beyond the official inputs is associated with offer outcomes.

\textit{What must be excluded from Cut 2.} The lottery number itself (and any variable that is a deterministic function of it, such as offer outcomes at other schools in the same round) must never enter Cut 2. Including the lottery realization would trivially improve prediction and tell us nothing about mechanism integrity. Everything else --- any pre-lottery student or school characteristic --- is fair game.

\section{Data and Setting}

We use student-level administrative data from Denver Public Schools. The data contain submitted preference lists, priority structures (at the bucket level), lottery numbers, first-round offer outcomes, and a rich set of student demographics and academic records. We also observe neighborhood-level characteristics and school-level attributes. This combination gives us both the complete mechanism inputs needed to compute $\hat{p}^{DA}$ (and to construct Cut 1 features) and the auxiliary covariates needed for Cut 2.

The outcome variable $D_i(s)$ is the first-round DA offer indicator --- whether student $i$ was offered a seat at school $s$ in the initial round of SchoolChoice. We focus on first-round offers because only the first round uses DA; the second round is an ad hoc school-by-school process unrelated to the lottery.

\comment{[To be filled: sample description, years, grades, summary statistics, bucket structure details.]}

\section{Methodology}

\subsection{Computing $\hat{p}^{DA}$}

We compute the benchmark propensity score by simulation: for each market (defined by year and entry grade), we draw lottery numbers $10^6$ times, run the exact DPS first-round DA algorithm (including bucket splits, family link, and seat reservations), and record average offer rates by student type and bucket. These simulated scores converge to the true finite-market propensity score by the law of large numbers.

\subsection{Prediction Task}

For each student-school pair $(i, s)$ where student $i$ has ranked school $s$, we predict the binary outcome $D_i(s) \in \{0, 1\}$ indicating whether $i$ received a first-round offer at $s$. The predicted probability is our ML propensity score $\hat{p}^{ML}$.

\subsection{Model}

We use gradient-boosted decision trees (XGBoost) to predict $D_i(s)$. Gradient boosting is the standard ML method for structured prediction tasks in applied economics and has been used extensively in recent work on prediction policy problems (Kleinberg et al., 2018; Mullainathan and Spiess, 2017; Ludwig and Mullainathan, 2024; Athey and Imbens, 2019). It handles mixed feature types (categorical priorities, ordinal ranks, continuous distances), captures the nonlinearities and interactions inherent in the DA propensity score, and produces well-calibrated predicted probabilities. Crucially, if there is any additional explanatory power in the exhaust covariates after conditioning on the mechanism inputs, gradient boosting is flexible enough to find it.

We use $K$-fold cross-fitting: the model is trained on $K-1$ folds and predictions are generated for the held-out fold, so that each student's predicted probability is an out-of-sample prediction. We train the model twice --- once on Cut 1 features (producing $\hat{p}^{ML}_1$) and once on Cut 2 features (producing $\hat{p}^{ML}_2$) --- and compare their predictive accuracy.

\subsection{Features}

\textit{Cut 1 features} (mechanism inputs): rank of school $s$ on student $i$'s preference list; priority group of $i$ at each bucket of $s$; capacity of each bucket; total number of applicants to $s$ by priority group; number of schools ranked by $i$; identities of schools ranked; neighborhood school indicator.

\textit{Cut 2 features} (mechanism inputs $+$ exhaust): all Cut 1 features plus race/ethnicity, gender, free/reduced lunch eligibility, prior standardized test scores (math, reading), home-to-school distance, census tract characteristics (median income, poverty rate, educational attainment), school-level characteristics (enrollment, demographic composition, prior achievement levels).

\subsection{Evaluation}

We evaluate predictive accuracy using mean squared error (MSE) over student-school pairs:
\[
\text{MSE} = \frac{1}{N} \sum_{i,s} \big( D_i(s) - \hat{p}(i,s) \big)^2,
\]
where the sum is over all pairs $(i,s)$ in which student $i$ ranked school $s$, $D_i(s) \in \{0,1\}$ is the realized offer outcome, and $\hat{p}(i,s)$ is the predicted offer probability. Because the outcome is binary, MSE is minimized by the true conditional probability $\Pr[D_i(s) = 1 \mid X_i]$, making it a proper scoring rule for evaluating probability forecasts. We compare:
\begin{itemize}
    \item $\text{MSE}(\hat{p}^{ML}_1)$ vs.\ $\text{MSE}(\hat{p}^{DA})$: Does ML recover the mechanism's propensity score? (Calibration check.)
    \item $\text{MSE}(\hat{p}^{ML}_2)$ vs.\ $\text{MSE}(\hat{p}^{ML}_1)$: Does the exhaust improve prediction? (The test.)
\end{itemize}

\section{Results}

\comment{[To be filled: actual results, tables, figures.]}

\noindent Three families of results are possible:

\subsection{$\hat{p}^{ML}_2 \approx \hat{p}^{ML}_1$: No Deviation Detected}

The exhaust covariates add nothing on top of the mechanism inputs. The DA propensity score $\hat{p}^{DA}$, as computed by the researcher from the administrative data, is well-calibrated to observed offers. The MDRD assumptions are validated \textit{empirically}, not merely maintained. For a PNAS audience, the message is direct: ``centralized assignment really does function like an experiment, and the propensity scores that underpin causal inference in this setting are reliable.''

\subsection{$\hat{p}^{ML}_2 > \hat{p}^{ML}_1$: Deviation Detected}

The exhaust covariates improve prediction of offer outcomes beyond what the mechanism inputs alone provide. Something beyond the official inputs is associated with offers. This finding admits a hierarchy of explanations, from most mundane to most consequential:

\begin{enumerate}
    \item \textbf{Measurement error in $\theta$.} The administrative data may misrecord priorities (e.g., a sibling enrollment not updated, an address change not reflected, an FRL eligibility flag processed late). In this case, the exhaust covariates are proxying for the \textit{correct} $\theta$ that the mechanism actually used --- the ML audit has detected that the researcher's recorded inputs do not match what the algorithm saw.

    \item \textbf{Mechanism misspecification.} The researcher's simulation of DA may not perfectly replicate the implemented rules --- incorrect bucket construction, missing seat reservation logic, unaccounted-for family link provisions. The deviation reflects a gap in the researcher's model, not in the district's implementation.

    \item \textbf{Operational discretion.} Districts may make documented or semi-documented adjustments after the algorithm runs: late applications processed manually, casework placements, principal-requested exceptions. These are ``on the books'' but outside DA.

    \item \textbf{Systematic undocumented dependence.} Offers depend on student characteristics (race, income, test scores) in ways that are neither part of the stated mechanism nor documented as exceptions. This is the strongest finding --- evidence that the district is putting a thumb on the scale.
\end{enumerate}

\noindent Importantly, \textit{all four findings are useful}. The first two tell the researcher that $\hat{p}^{DA}$ needs to be recomputed before using it for causal inference. The third and fourth tell the district that its assignment process departs from its stated rules. Examining which variables drive the improvement in Cut 2 --- and at which schools and for which students --- helps distinguish among these explanations.

\subsection{$\hat{p}^{ML}_1 \not\approx \hat{p}^{DA}$: ML Cannot Recover $\hat{p}^{DA}$}

If the calibration check fails, the mechanism inputs in our data do not fully encode $\theta$. With the Denver data, where we observe complete preference lists and priority structures at the bucket level, this is unlikely. But in other districts where the researcher has incomplete mechanism inputs, this finding would signal that the available data are insufficient to reconstruct the assignment probability function --- useful to know before attempting causal inference.

\section{Policy Implications}

This framework provides an accountability tool that scales. Any district that runs a centralized assignment mechanism and collects student-level data on offers and covariates can be audited, even without access to the mechanism's source code. The audit detects discrepancies between stated and implemented assignment rules --- whether those discrepancies arise from data quality problems, implementation complexity, or deliberate intervention.

For researchers, the audit is a prerequisite for credible MDRD-based causal inference: it tests the assumption on which everything else depends. For districts, it provides a diagnostic: where and for whom does the implemented assignment process depart from the algorithm?

\section{Conclusion}

\comment{[To be filled.]}

\newpage
\printbibliography

% \newpage
% \appendix
% \section{Appendix}

\end{document}
